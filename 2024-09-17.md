# 2024-09-17

## 想到了些啥

发现自己好像没太多时间刷题。可能刷题作为一种长期保持手感的策略会更好。

更重要的是，不希望自己困在数学里。学习数学的最终目标是理解机器学习的内容，所以正确的方法应该是尽早温习完数学，然后就去学机器学习的内容。

## 今天做了啥

- 线性代数视频课：学完了：
  - 第十五章：讲了特征值的一个计算技巧
  - 第十六章：最屌的一章！！！！讲了抽象线性代数

## 需要记点啥

### 对角矩阵特征值的快速计算方法

对角矩阵是一种除了主对角线之外其他地方均为0的矩阵。如昨天所记，它有很多好用的性质。

对于一个$N$维的对角矩阵，可以记为$diag(a_1,a_2,...a_N)$

对角矩阵有一个好用的性质就是，它的特征值就是它的对角数组的值的去重集。

### 二维矩阵特征值的快速计算方法

首先需要知道两个规则：

- 矩阵的迹等于矩阵的特征值之和。对于二维矩阵，则有：$tr(\begin{bmatrix}a&b\\c&d\end{bmatrix})=a+d=\lambda_1+\lambda_2$
  - 换言之，就有：$\frac{1}{2}tr(\begin{bmatrix}a&b\\c&d\end{bmatrix})=\frac{a+d}{2}=\frac{\lambda_1+\lambda_2}{2}=mean(\lambda_1,\lambda_2)$
- 二维矩阵的特征值的积等于矩阵的行列式值，即：$\det(\begin{bmatrix}a&b\\c&d\end{bmatrix})=\lambda_1 \lambda_2$

那么在知道两个特征值的均值$m$和积$p$时，就可以求这两个值的数值了。

首先假设$\lambda_1=m-d$，显然就有$\lambda_2=m+d$

那么就有$p=\lambda_1 \lambda_2=m^2-d^2$，解得$d=\sqrt{m^2-p}$

于是可得：$\lambda=m\pm\sqrt{m^2-p}$

#### 规则的暴力证明

根据特征值定义可知，特征值为$\det(\begin{bmatrix}a-\lambda&b\\c&d-\lambda\end{bmatrix})=0$的解

拆解如下：
$$
\begin{align*}
(a-\lambda)(d-\lambda)-bc=0\\
ad-(a+d)\lambda+\lambda^2-bc=0\\
\lambda^2+(-a-d)\lambda+(ad-bc)=0
\end{align*}
$$
可得：
$$
\begin{align*}
&\lambda_1=\frac{(a+d)+\sqrt{(a+d)^2-4(ad-bc)}}{2}\\
&\lambda_2=\frac{(a+d)-\sqrt{(a+d)^2-4(ad-bc)}}{2}
\end{align*}
$$
于是就有：
$$
\begin{align*}
&\lambda_1 + \lambda_2 = \frac{2(a+d)}{2}=a+d \\
&\lambda_1 \lambda_2 = \frac{(a+d)^2-((a+d)^2-4(ad-bc))}{4}=ad-bc
\end{align*}
$$
上式得证。

### 抽象线性空间

**这正是线性代数的魅力啊！！！**

回到一开始的问题：向量是什么？

回顾前面的笔记，我们一直把向量当成是几何空间上的一个箭头，甚至是特定的二维空间里的箭头，并依次赋予各种线性代数概念几何意义，以便于更好地理解他们。

但，向量真的只可以是箭头吗？

**不是的**

只要某个东西满足下图的公理（可加性，数乘），我们都可以将其称之为一种向量：

![](\img\screenshot-20240917-160824.png)

举个很不可思议的例子：函数

我们完全可以把函数看作是有若干维的向量，每个维度的值是函数的采样，它就是一个无穷长的向量。

于是你可以发现，函数完全满足上述性质。

在这个视角上，你会发现**对函数求导**也是一种线性变换。因为这个操作满足可加性和数乘。

甚至我们可以换一个定义方向：

对于多项式函数而言，当我们定义这么一个基：
$$
\begin{bmatrix}1&x&x^2&x^3&\cdots\end{bmatrix}
$$
那么我们也可以将多项式函数转换为向量，以$f(x)=2x^3-5x^2+9$为例，按照上述规则可以将其转化为向量：
$$
\begin{bmatrix}9&0&-5&2&0&0&\cdots\end{bmatrix}^T
$$
你会发现它依旧满足上述公理。

并且我们还可以将求导这个线性变换给具现化出来：
$$
D=\begin{bmatrix}
0&1&0&0&0&0&\cdots\\
0&0&2&0&0&0&\cdots\\
0&0&0&3&0&0&\cdots\\
0&0&0&0&4&0&\cdots\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\ddots
\end{bmatrix}
$$
于是对$f(x)$求导便可以转换为如下计算方式：
$$
\frac{\mathrm{d}f(x)}{\mathrm{d}x}=D\vec{f}=
\begin{bmatrix}
0&1&0&0&0&0&\cdots\\
0&0&2&0&0&0&\cdots\\
0&0&0&3&0&0&\cdots\\
0&0&0&0&4&0&\cdots\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\ddots
\end{bmatrix}
\begin{bmatrix}
9\\
0\\
-5\\
2\\
\vdots
\end{bmatrix}
=
\begin{bmatrix}
0\\
-10\\
6\\
0\\
\vdots
\end{bmatrix}
\rightarrow
6x^2-10x
$$
然后你会发现，很多微积分的东西都可以对应到线性代数的概念里（如特征函数对应特征值）

更广义地，**只要定义的空间满足向量空间的定义，它就可以应用所有线性代数的概念。**

重复一遍，**这正是线性代数的魅力啊！！！**

## 线代学完了，接着呢

本来想接着学高数的，但是发现高数的学习资料好像不是很足。看起来像模像样的可汗学院却是全英文，，看着很费力。

目前接下来的计划可以是：

- 持续刷数学题，目标是把买来的练习册做完，权当巩固数学
- 把3b1b的高数和概率论刷完，之后就可以顺着看机器学习了
- 找了cs自学指南的资料，可以在之后抽时间把数学补上，但不要耽误学机器学习的进度

