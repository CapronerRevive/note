# 2024-09-08

## 想到了些啥

突然想起，这次又双叒叕重整旗鼓开始学习还是有些子目标的：想要跳到游戏行业去。
感觉自己并不喜欢现在所在的业务，虽说也能得过且过，但肯定是还想冲一冲的。

大致看了一下心仪的岗位，大致上需要这些技能：
- 任选一个编程语言：C/C++，Python，Go
- 机器学习基础
- 任选一个训练框架：tf，torch，caffe
  - 最好知道怎么多卡训练加速
- 推理引擎：TensorRT，ncnn
  - 最好知道怎么优化算子
- 了解分布式系统原理
- 懂一些云相关基础（k8s，docker）
- 懂简单的一站式开发（django+flask）

整理了一下，我需要准备的：
- AI相关：
  - 机器学习基础：**重中之重**，这一块我一直缺乏系统性学习，我需要找个清单系统性学习并自己实践出点东西
    - 结合自己的工作项目，我需要从机器学习基础学到LLM，并以自己手训小型NLP网络为最终目标
  - 训练框架：选择torch，不需要掌握太深，以弄清原理并手训为目标即可
  - 推理引擎：重点，我需要深入了解TensorRT，目标是将上述网络进行部署并推理加速
    - 可以考虑TRT+Triton这条路子
- 编程语言：需要过一遍C++，同时了解py和go。所幸这些都鼓捣过，可以不用太投入精力
- 后端开发：有一些东西必须了解，过一遍原理和八股文即可，有空可以适当实操
  - k8s：了解原理
  - docker：了解原理，知道咋弄镜像啥的
  - django + flask：了解原理，有空的话用这套方式搭一个小网站
  - 分布式系统：了解原理，这些或许会跟k8s一起学
- 一些加分的技能：
  - 多卡训练加速
  - 算子优化：**重要**

整理下来的路子其实就是：
1. 系统学LLM
2. 捡起后端开发
3. 深入学习推理加速

嗯，先这样吧，能坚持多久都另说。

## 今天做了啥

- 可汗习题：刷完了，一共就没多少习题，回去等考研资料吧
- 视频课：继承昨天的进度，学了：
  - 第一章：向量，大致知道了向量是个啥
  - 第二章：还是向量，讲了线性组合、线性空间、基底向量
  - 第三章：线性变换：从线性变换的角度切入矩阵这个思路很有意思

## 需要记点啥

### 从线性变换到矩阵

首先需要知道何为线性变换。

#### 基于坐标系谈线性变换

从二维空间的角度来看，当我对整个二维空间本身进行满足如下三个条件的变换时，就是线性变换：
1. 变换前后原点位置不变
2. 变换前后行列的间距不变
3. 变换前后坐标系都是直线

其实可以想象一下，把坐标系的所有的直线看做是可伸缩的长条，每个交叉点都有颗螺丝固定的，原点焊死在一个地方。这样的话你对它做任何变化都是线性变换。

或者换一种想象，想象一个若干条钢丝组成一个每个网格都是1x1大小的，无限大的网，这个网浮空着正对着你，且原点依旧焊死在空间里的某个点。

这个时候你可以对这个网做任何方式地旋转，包括三维地方式，你也可以在任意角度任意远近地观察。

这种旋转会让网格们看起来可大可小，可以变成长方形，可以变成平行四边形，甚至变成直线。但它的所有变化都逃不出线性变换的限制。

基于上可以看出，**线性变换是对于整个线性空间而言，而非对某一向量而言。**

#### 我们如何描述线性变换

首先我们知道，对于一个二维空间，只需要有两个线性无关的基底向量就可以描述整个空间。

那么对于线性变换而言，本质上就是对这两个基底向量进行替换。

我们可以这么描述线性变换：对于基底向量i，j而言，任意线性变换的本质就是将向量进行替换：

$$
\begin{matrix} 
  i: \begin{bmatrix}1\\
  0\end{bmatrix} \longrightarrow \begin{bmatrix}a\\
  b\end{bmatrix}\\  
  j: \begin{bmatrix}0\\
  1\end{bmatrix} \longrightarrow \begin{bmatrix}c\\
  d\end{bmatrix}
\end{matrix}
$$

#### 给定任意向量，如何知道其线性变换后的向量

我们知道，对于任意二维向量，其第一维表示基底向量i的缩放倍数，第二维表示基底向量j的缩放倍数。

那么，当我们知道线性变换的本质就是基底向量的替换之后，对于任意二维向量(x, y)而言，线性变换后的向量值即为：

$$
x\begin{bmatrix}
 a\\
b
\end{bmatrix} + y\begin{bmatrix}
 c\\
d
\end{bmatrix}
$$

#### 转换成矩阵计算

于是我们可以发现，我们完全可以把任意一个二维的线性变换方式总结为一个2x2的矩阵，对于任意二维向量，求其变换后向量的计算本质上就是矩阵乘向量：

$$
\begin{bmatrix}
a  & c\\
b  & d
\end{bmatrix}\begin{bmatrix}
x \\
y
\end{bmatrix}
$$
