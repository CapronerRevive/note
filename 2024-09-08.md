# 2024-09-08

## 想到了些啥

突然想起，这次又双叒叕重整旗鼓开始学习还是有些子目标的：想要跳到游戏行业去。
感觉自己并不喜欢现在所在的业务，虽说也能得过且过，但肯定是还想冲一冲的。

大致看了一下心仪的岗位，大致上需要这些技能：
- 任选一个编程语言：C/C++，Python，Go
- 机器学习基础
- 任选一个训练框架：tf，torch，caffe
  - 最好知道怎么多卡训练加速
- 推理引擎：TensorRT，ncnn
  - 最好知道怎么优化算子
- 了解分布式系统原理
- 懂一些云相关基础（k8s，docker）
- 懂简单的一站式开发（django+flask）

整理了一下，我需要准备的：
- AI相关：
  - 机器学习基础：**重中之重**，这一块我一直缺乏系统性学习，我需要找个清单系统性学习并自己实践出点东西
    - 结合自己的工作项目，我需要从机器学习基础学到LLM，并以自己手训小型NLP网络为最终目标
  - 训练框架：选择torch，不需要掌握太深，以弄清原理并手训为目标即可
  - 推理引擎：重点，我需要深入了解TensorRT，目标是将上述网络进行部署并推理加速
    - 可以考虑TRT+Triton这条路子
- 编程语言：需要过一遍C++，同时了解py和go。所幸这些都鼓捣过，可以不用太投入精力
- 后端开发：有一些东西必须了解，过一遍原理和八股文即可，有空可以适当实操
  - k8s：了解原理
  - docker：了解原理，知道咋弄镜像啥的
  - django + flask：了解原理，有空的话用这套方式搭一个小网站
  - 分布式系统：了解原理，这些或许会跟k8s一起学
- 一些加分的技能：
  - 多卡训练加速
  - 算子优化：**重要**

整理下来的路子其实就是：
1. 系统学LLM
2. 捡起后端开发
3. 深入学习推理加速

嗯，先这样吧，能坚持多久都另说。

## 今天做了啥

- 可汗习题：刷完了，一共就没多少习题，回去等考研资料吧
- 视频课：继承昨天的进度，学了：
  - 第一章：向量，大致知道了向量是个啥
  - 第二章：还是向量，讲了线性组合、线性空间、基底向量
  - 第三章：线性变换：从线性变换的角度切入矩阵这个思路很有意思
  - 第四章：矩阵乘法：依旧是线性变换的角度
  - 第五章：将上述情况从二维拓展到三维
  - 第六章：行列式，讲了行列式的几何意义，也是一个有意思的角度
  - 第七章：从几何角度上讲了逆矩阵，列空间，秩，零空间。但没有讲计算方法
  - 第八章：补充了非平方矩阵的几何意义
  - 第九章：讲了点积
  - 第十、十一章：讲了叉积
- 发现视频课下面是有练习的，顺带刷了

## 需要记点啥

### 从线性变换到矩阵

> 这里只讨论2x2矩阵

首先需要知道何为线性变换。

#### 基于坐标系谈线性变换

从二维空间的角度来看，当我对整个二维空间本身进行满足如下三个条件的变换时，就是线性变换：
1. 变换前后原点位置不变
2. 变换前后行列的间距不变
3. 变换前后坐标系都是直线

其实可以想象一下，把坐标系的所有的直线看做是可伸缩的长条，每个交叉点都有颗螺丝固定的，原点焊死在一个地方。这样的话你对它做任何变化都是线性变换。

或者换一种想象，想象一个若干条钢丝组成一个每个网格都是1x1大小的，无限大的网，这个网浮空着正对着你，且原点依旧焊死在空间里的某个点。

这个时候你可以对这个网做任何方式地旋转，包括三维地方式，你也可以在任意角度任意远近地观察。

这种旋转会让网格们看起来可大可小，可以变成长方形，可以变成平行四边形，甚至变成直线。但它的所有变化都逃不出线性变换的限制。

基于上可以看出，**线性变换是对于整个线性空间而言，而非对某一向量而言。**

#### 我们如何描述线性变换

首先我们知道，对于一个二维空间，只需要有两个线性无关的基底向量就可以描述整个空间。

那么对于线性变换而言，本质上就是对这两个基底向量进行替换。

我们可以这么描述线性变换：对于基底向量$(i, j)$而言，任意线性变换的本质就是将向量进行替换：

$$
\begin{matrix} 
  i: \begin{bmatrix}1\\
  0\end{bmatrix} \longrightarrow \begin{bmatrix}a\\
  b\end{bmatrix}\\  
  j: \begin{bmatrix}0\\
  1\end{bmatrix} \longrightarrow \begin{bmatrix}c\\
  d\end{bmatrix}
\end{matrix}
$$

#### 给定任意向量，如何知道其线性变换后的向量

我们知道，对于任意二维向量，其第一维表示基底向量$i$的缩放倍数，第二维表示基底向量j的缩放倍数。

那么，当我们知道线性变换的本质就是基底向量的替换之后，对于任意二维向量$(x, y)$而言，线性变换后的向量值即为：

$$
x\begin{bmatrix}
 a\\
b
\end{bmatrix} + y\begin{bmatrix}
 c\\
d
\end{bmatrix}
$$

#### 转换成矩阵计算

于是我们可以发现，我们完全可以把任意一个二维的线性变换方式总结为一个2x2的矩阵，对于任意二维向量$(x, y)$，求其变换后向量的计算本质上就是矩阵乘向量：

$$
\begin{bmatrix}
a  & c\\
b  & d
\end{bmatrix}\begin{bmatrix}
x \\
y
\end{bmatrix}
$$

### 从线性变换到矩阵乘法

> 这里只讨论2x2矩阵

基于上面的视角，我可以便可以认为，两个矩阵相乘可以看作是两次线性变换的复合。

对于一个矩阵乘：$M=M_2M_1$，可以看作是，线性空间先进行$M_1$的线性变换，再进行$M_2$的线性变换。而这跟直接进行$M$的线性变换是等效的。

值得注意的是，不论是单次线性变换还是多次线性变换，线性变换的顺序都是**从右向左的**。3b1b认为这是跟复合函数的写法保持一致（如$f(g(h(x)))$）。

基于上述视角便可以理解矩阵乘的交换律和结合律：

- 交换律：交换律在矩阵乘法上不成立。因为对于线性变换而言，先$A$后$B$跟先$B$后$A$是不一样的
  - 一个例子，先纵向拉伸2倍再旋转90度，跟先旋转90度再纵向拉伸2倍，显然是不一样的。原因是旋转先后导致被拉伸的基底向量不同。
- 结合律：不论怎么交换结合的方式，矩阵乘最终的计算都是从右向左的，这不会影响对应线性变换的顺序

### 几何意义上的行列式

从二维线性空间的角度来讲，一个矩阵的行列式等同于该矩阵所表达的线性变换里，**两个基底向量围成的空间的面积**。

于是我们可以很好理解：

- 行列式的值表示这个线性变换对整个空间的缩放程度
- 如果行列式的值为零，表示线性变换后的空间可以被降维
- 如果行列式的值为负，相当于基底向量的相对位置被交换了
  - 从二维的角度上讲，相当于整个二维空间被翻面了。

基于这个视角，行列式的分配律就可以很好理解了，本质上就是面积缩放可以连乘：
$$
\det(M_1M_2)=\det(M_1)\det(M_2)
$$

#### 行列式如何计算

这里只做个备忘，根据视频课的观点，计算过程不重要

对于二维矩阵，行列式的值为：
$$
\det(\begin{bmatrix}a&b\\c&d\end{bmatrix})=ad-bc
$$
对于高维矩阵，行列式的值可以降维计算。以三维为例：
$$
\det(\begin{bmatrix}a_1&a_2&a_3\\b_1&b_2&b_3\\c_1&c_2&c_3\end{bmatrix})
=
a_1\det(\begin{bmatrix}b_2&b_3\\c_2&c_3\end{bmatrix})
-
a_2\det(\begin{bmatrix}b_1&b_3\\c_1&c_3\end{bmatrix})
+
a_3\det(\begin{bmatrix}b_1&b_2\\c_1&c_2\end{bmatrix})
$$
从上述计算过程可以看出，$N$阶行列式计算的朴素时间复杂度为$O(N!)$，指望行列式来判断矩阵是否可以降维是不实际的。

### 几何意义上的矩阵概念

#### 逆矩阵

这个比较好理解。在我们求$A\vec{x}=\vec{v}$时，需要转换为$\vec{x}=A^{-1}\vec{v}$。从几何意义上讲，$\vec{v}$是$\vec{x}$经过线性变换$A$得到的，所以如果要反过来求$\vec{x}$的话本质上就是在求逆的线性变换。

当$A$对应的线性变换没有降维的情况时，显然逆矩阵唯一。

当$A$对应的线性变换出现降维的情况时，逆矩阵不存在，此时$\vec{x}$的解需要看$\vec{v}$：

- $\vec{v}$落在$A$对应的线性变换的空间里（即列空间）：此时$\vec{x}$有无数个解，因为此时$A$的逆变换有无数个
- $\vec{v}$不落在$A$的列空间里：此时$\vec{v}$不可能通过线性变换$A$得到，故$\vec{x}$无解

##### 如何计算

将矩阵$A$拼上单位矩阵做增广，然后进行行初等变换将左矩阵变为单位矩阵，则右矩阵为$A^{-1}$

下例：
$$
A=\begin{bmatrix}1 & 2\\-1 & -3\end{bmatrix}\\
\\
EXTEND\_A=\begin{bmatrix}1 & 2 & 1 & 0\\-1 & -3 & 0 & 1\end{bmatrix}\\
\rightarrow\begin{bmatrix}1 & 2 & 1 & 0\\0 & -1 & 1 & 1\end{bmatrix}\\
\rightarrow\begin{bmatrix}1 & 2 & 1 & 0\\0 & 1 & -1 & -1\end{bmatrix}\\
\rightarrow\begin{bmatrix}1 & 0 & 3 & 2\\0 & 1 & -1 & -1\end{bmatrix}\\
\\
A^{-1}=\begin{bmatrix}3 & 2\\-1 & -1\end{bmatrix}\\
$$


#### 列空间

从几何意义上看，矩阵$A$的列空间就是其代表的线性变换所指向的空间。

从矩阵表示来看，矩阵$A$的列空间为一个由$A$中所有线性无关的列组成的矩阵。

#### 秩

从几何上看，这个就是$A$的列空间的维度了

##### 如何计算

通过高斯消元法，将矩阵通过初等行变换转换为上三角矩阵，然后看非零行的数量

下例：
$$
A=\begin{bmatrix}2&4&1&3\\-1&-2&1&0\\0&0&2&2\\3&6&2&5\end{bmatrix}\\
\rightarrow\begin{bmatrix}1&2&0.5&1.5\\-1&-2&1&0\\0&0&2&2\\3&6&2&5\end{bmatrix}\\
\rightarrow\begin{bmatrix}1&2&0.5&1.5\\0&0&1.5&1.5\\0&0&2&2\\3&6&2&5\end{bmatrix}\\
\rightarrow\begin{bmatrix}1&2&0.5&1.5\\0&0&1.5&1.5\\0&0&2&2\\0&0&0.5&0.5\end{bmatrix}\\
\rightarrow\begin{bmatrix}1&2&0.5&1.5\\0&0&1&1\\0&0&2&2\\0&0&0.5&0.5\end{bmatrix}\\
\rightarrow\begin{bmatrix}1&2&0.5&1.5\\0&0&1&1\\0&0&0&0\\0&0&0.5&0.5\end{bmatrix}\\
\rightarrow\begin{bmatrix}1&2&0.5&1.5\\0&0&1&1\\0&0&0&0\\0&0&0&0\end{bmatrix}\\
\\
\rank(A)=2
$$


#### 零空间

从几何上看，这个就是所有**在经过线性变换**$A$**后变为零向量**的向量所组成的空间。

由上可知，$A$的列空间维度+$A$的零空间维度=$A$的维度

### 几何意义上的非平方矩阵

如果说平方矩阵的是某个维度的线性变换，那么非平方矩阵就是从a维度的空间到b维度空间的线性变换。

考虑如下两种变换：

- 低维空间（R=a）变换到高维空间（R=b）：该线性变换进行后的矩阵所表示的空间维度必然小于等于a
  - 因为高维的每个基底向量都是由低维空间的基底向量进行线性组合得到的，那么必然就最多只有a个线性无关向量
- 高维空间（R=b）变换到低维空间（R=a）：该线性变换可以看作是高维空间在低维上的映射

### 几何意义上的点积

先复习一下点积：
$$
\begin{bmatrix}a\\b\end{bmatrix}\dotproduct\begin{bmatrix}c\\d\end{bmatrix}=ac+bd
$$


简单地理解是，点积等于向量a在向量b上的投影的长度，乘以向量b的长度。

但根据上面的非平方矩阵的几何意义，我们也可以将点积稍作变换，转变为从多维空间到一维空间（数轴）的映射：
$$
\begin{bmatrix}a\\b\end{bmatrix}\dotproduct\begin{bmatrix}c\\d\end{bmatrix}=\begin{bmatrix}a&b\end{bmatrix}\begin{bmatrix}c\\d\end{bmatrix}=ac+bd
$$

### 几何意义上的叉积

> 很神奇地，我目前能查到的资料只讲了二维叉积和三维叉积，看起来这个概念并不是很好地向高维拓展

这里需要分两个情况来讲：二维叉积和三维叉积

#### 二维叉积

二维叉积的几何意义是两个向量围起来的四边形的有向面积，计算方式跟行列式是一样的：
$$
\begin{bmatrix}a\\b\end{bmatrix}\cross\begin{bmatrix}c\\d\end{bmatrix}=\det(\begin{bmatrix}a&c\\b&d\end{bmatrix})=ad-bc
$$

#### 三维叉积

三维叉积就复杂了，其叉积的结果为一个三维的向量，该向量满足如下条件：

- 长度等于两个向量围成的四边形的面积
- 方向遵循右手定则垂直于两个向量
  - 右手定则用于规则三维坐标系，x轴为食指，y轴为中指，z轴为拇指

其计算过程为：
$$
\begin{array}{l}
\begin{bmatrix}a_1\\b_1\\c_1\end{bmatrix}\cross\begin{bmatrix}a_2\\b_2\\c_2\end{bmatrix}\\
=\det(\begin{bmatrix}\hat{i}&a_1&a_2\\\hat{j}&b_1&b_2\\\hat{k}&c_1&c_2\end{bmatrix})\\
=\hat{i}\det(\begin{bmatrix}b_1&b_2\\c_1&c_2\end{bmatrix})-a_1\det(\begin{bmatrix}\hat{j}&b_2\\\hat{k}&c_2\end{bmatrix})+
a_2\det(\begin{bmatrix}\hat{j}&b_1\\\hat{k}&c_1\end{bmatrix})\\
=\hat{i}\det(\begin{bmatrix}b_1&b_2\\c_1&c_2\end{bmatrix})
-a_1(c_2\hat{j}-b_2\hat{k})
+a_2(c_1\hat{j}-b_1\hat{k})\\
=\hat{i}\det(\begin{bmatrix}b_1&b_2\\c_1&c_2\end{bmatrix})
-a_1c_2\hat{j}+a_1b_2\hat{k}
+a_2c_1\hat{j}-a_2b_1\hat{k}\\
=\hat{i}(b_1c_2-b_2c_1)+
\hat{j}(a_2c_1-a_1c_2)+
\hat{k}(a_1b_2-a_2b_1)\\
=\begin{bmatrix}b_1c_2-b_2c_1\\a_2c_1-a_1c_2\\a_1b_2-a_2b_1\end{bmatrix}
\end{array}
$$
看着挺复杂的，它的几何意义也不太好解释。这里试图总结一下：

首先，我们假定三维叉积可以跟二维叉积一样，用三个三维向量叉积得到一个值（虽然不能这么算），根据二维向量的计算方法，这个假定叉积的值就是三个三维向量组成的矩阵的行列式，也就是其围起来的立方体的体积。
$$
\begin{array}{l}
(just\ fake)\\
\begin{bmatrix}a_0\\b_0\\c_0\end{bmatrix}(fake)\cross\begin{bmatrix}a_1\\b_1\\c_1\end{bmatrix}\cross\begin{bmatrix}a_2\\b_2\\c_2\end{bmatrix}=\det(\begin{bmatrix}a_0&a_1&a_2\\b_0&b_1&b_2\\c_0&c_1&c_2\end{bmatrix})
\end{array}
$$
于是，我们期望后两个三维向量的叉积是一个向量，该向量需要满足：

- 假设的向量在该向量上的投影是该立方体的高
- 该向量的长度是该立方体的底面积

于是你会发现，这个假设出来的叉积的计算方式正好能满足点积：
$$
\begin{array}{l}
\begin{bmatrix}a_0\\b_0\\c_0\end{bmatrix}\dotproduct(\begin{bmatrix}a_1\\b_1\\c_1\end{bmatrix}\cross\begin{bmatrix}a_2\\b_2\\c_2\end{bmatrix})=\det(\begin{bmatrix}a_0&a_1&a_2\\b_0&b_1&b_2\\c_0&c_1&c_2\end{bmatrix})
\end{array}
$$
根据一系列冗长的计算，可以证明叉积结果就是上面的值。
